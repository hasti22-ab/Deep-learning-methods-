# -*- coding: utf-8 -*-
"""Q&A2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-63zbO9ax7B5KWrLzhBpOLuear_X28BQ
"""

!pip install python-docx transformers

from google.colab import files
uploaded = files.upload()

!pip install python-docx

import docx

# Define the function to read content from a .docx file
def read_word_file(file_path):
    doc = docx.Document(file_path)
    full_text = []
    for para in doc.paragraphs:
        full_text.append(para.text)
    return '\n'.join(full_text)

# Example usage (assuming you've already uploaded a file)
file_name = 'Document.docx'  # Replace with your file's name
doc_content = read_word_file(file_name)

# Print the document content (optional)
print(doc_content)

!pip install transformers torch

!pip install transformers datasets

from datasets import load_dataset

# Load the SQuAD dataset (subset to minimize memory use)
squad_streamed = load_dataset('squad', split='train', streaming=True)

# Stream and inspect one example (useful for large datasets)
for example in squad_streamed.take(1):
    print(example)

def preprocess_function(example):
    # Tokenize without return_offsets_mapping
    tokenized_example = tokenizer(
        example['question'],
        example['context'],
        max_length=384,
        truncation=True,
        padding="max_length"
    )

    # Assuming the 'answers' field is present and well-formed
    start_char = example['answers']['answer_start'][0]
    end_char = start_char + len(example['answers']['text'][0])

    # You need to adjust how you get start/end tokens without offsets
    # Direct tokenization of the start/end text
    answer = example['answers']['text'][0]
    answer_token_ids = tokenizer.encode(answer, add_special_tokens=False)

    # Search for the answer's tokenized form within the context
    context_token_ids = tokenized_example['input_ids']
    start_idx = context_token_ids.index(answer_token_ids[0])  # Find where answer starts
    end_idx = start_idx + len(answer_token_ids) - 1  # Set where the answer ends

    # Add start and end positions to the tokenized example
    tokenized_example["start_positions"] = start_idx
    tokenized_example["end_positions"] = end_idx

    return tokenized_example

!pip install accelerate

!pip install deepspeed

from accelerate import Accelerator
from transformers import DebertaV2Tokenizer, DebertaForQuestionAnswering, Trainer, TrainingArguments
from datasets import load_dataset
from torch.utils.data import DataLoader

# Load pre-trained tokenizer and model (use the fast tokenizer)
tokenizer = DebertaV2Tokenizer.from_pretrained('microsoft/deberta-v2-xlarge', use_fast=True)
model = DebertaForQuestionAnswering.from_pretrained('microsoft/deberta-v2-xlarge', ignore_mismatched_sizes=True)

# Load the SQuAD dataset in streaming mode
squad_streamed = load_dataset('squad', split='train', streaming=True)

# Training arguments (reduce per device batch size if memory is still an issue)
training_args = TrainingArguments(
    output_dir="./results",
    evaluation_strategy="epoch",
    learning_rate=3e-5,
    per_device_train_batch_size=2,  # You can adjust this if it's too large for memory
    per_device_eval_batch_size=2,
    num_train_epochs=3,
    weight_decay=0.01,
)

# Initialize the trainer without setting the dataset yet
trainer = Trainer(
    model=model,
    args=training_args
)

# Create an empty list for accumulating batches
current_batch = []

# Process the streamed dataset in batches
batch_size = 2  # Adjust based on memory capacity

for example in squad_streamed:
    # Preprocess the current example
    tokenized_example = preprocess_function(example)

    # Append to the current batch
    current_batch.append(tokenized_example)

    # Once we reach the batch size, we can train on this batch
    if len(current_batch) == batch_size:
        # Convert the list to a DataLoader (if needed, handle batching manually)
        data_loader = DataLoader(current_batch, batch_size=batch_size)

        # Pass the DataLoader to the Trainer and train on the batch
        trainer.train(data_loader)

        # Clear the batch after training
        current_batch = []

# Handle any remaining examples in the final batch
if current_batch:
    data_loader = DataLoader(current_batch, batch_size=len(current_batch))
    trainer.train(data_loader)

# Save the fine-tuned model and tokenizer
model.save_pretrained("./results/fine_tuned_model")
tokenizer.save_pretrained("./results/fine_tuned_tokenizer")

from transformers import DebertaForQuestionAnswering, Trainer, TrainingArguments
import torch

# Clear CUDA cache in case of memory fragmentation
torch.cuda.empty_cache()

# Load the pre-trained Deberta model with mismatched sizes
model = DebertaForQuestionAnswering.from_pretrained('microsoft/deberta-v2-xlarge', ignore_mismatched_sizes=True)

# Optimized training arguments
training_args = TrainingArguments(
    output_dir="./results",
    evaluation_strategy="epoch",
    learning_rate=3e-5,
    per_device_train_batch_size=1,       # Lower batch size
    per_device_eval_batch_size=1,
    gradient_accumulation_steps=4,       # Accumulate gradients to simulate a larger batch size
    num_train_epochs=2,
    weight_decay=0.01,
    fp16=True,                           # Use mixed precision to save memory
    logging_dir='./logs',
    logging_steps=10,
    save_steps=500,
    save_total_limit=2,
    report_to="none"
)

# Check for CUDA and use GPU if available
device = 'cuda' if torch.cuda.is_available() else 'cpu'
print(f'Using device: {device}')

# Initialize the Trainer class
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_squad['train'],
    eval_dataset=tokenized_squad['validation']
)

# Start fine-tuning the model
trainer.train()

# Optionally save the model and tokenizer
model.save_pretrained("./results/fine_tuned_deberta_model")
tokenizer.save_pretrained("./results/fine_tuned_deberta_tokenizer")

!rm -rf ~/.cache/huggingface

"""**Deberta**"""

from transformers import DebertaV2Tokenizer, DebertaForQuestionAnswering, pipeline
import torch
from sklearn.metrics import f1_score

# Force re-download the tokenizer and model, and allow for mismatched sizes
tokenizer = DebertaV2Tokenizer.from_pretrained('microsoft/deberta-v2-xlarge', force_download=True)
model = DebertaForQuestionAnswering.from_pretrained('microsoft/deberta-v2-xlarge', force_download=True, ignore_mismatched_sizes=True)



# Function to calculate F1 and exact match
def compute_exact_and_f1(pred_answer, true_answer):
    # Exact Match (EM) score
    exact_match = int(pred_answer == true_answer)

    # Tokenize both predicted and true answers for F1 score
    pred_tokens = tokenizer.tokenize(pred_answer)
    true_tokens = tokenizer.tokenize(true_answer)

    # Compute precision, recall, and F1 score
    common_tokens = set(pred_tokens) & set(true_tokens)
    if len(common_tokens) == 0:
        return exact_match, 0.0  # No overlap, F1 is zero

    precision = len(common_tokens) / len(pred_tokens)
    recall = len(common_tokens) / len(true_tokens)
    f1 = 2 * (precision * recall) / (precision + recall)

    return exact_match, f1

# Define question, context, and ground-truth answer
question = "What is XYZ Corporationâ€™s strategy to recover from the revenue decline?"
context = doc_content
true_answer = "Launching new products in emerging regions and investing in research and development in AI and renewable energy."

# Tokenize the input
inputs = tokenizer(question, context, return_tensors='pt')

# Forward pass through the model
with torch.no_grad():
    outputs = model(**inputs)

# Get the predicted start and end logits
start_logits, end_logits = outputs.start_logits, outputs.end_logits

# Get the predicted start and end positions
start_index = torch.argmax(start_logits)
end_index = torch.argmax(end_logits)

# Extract the predicted answer
tokens = inputs['input_ids'][0][start_index:end_index + 1]
pred_answer = tokenizer.decode(tokens, skip_special_tokens=True)

# Compute Exact Match and F1 score
exact_match, f1 = compute_exact_and_f1(pred_answer, true_answer)

# Display the results
print(f"Predicted Answer: {pred_answer}")
print(f"Exact Match (EM): {exact_match}")
print(f"F1 Score: {f1}")